{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to the Reddit API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a reddit instance\n",
    "reddit = praw.Reddit(client_id=\"EVgzl6l9hgixcX1QDiiN1g\",\n",
    "                     client_secret=\"wR1pXFdxhnpkBjockTDL3ykWGZsEDA\",\n",
    "                     user_agent=\"anything\",\n",
    "                     username=\"aryantiwari11\", \n",
    "                     password=\"taryan1234589\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"cryptomarkets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MESSAGE_PREFIX', 'STR_FIELD', 'VALID_TIME_FILTERS', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_convert_to_fancypants', '_create_or_update', '_fetch', '_fetch_data', '_fetch_info', '_fetched', '_kind', '_parse_xml_response', '_path', '_prepare', '_read_and_post_media', '_reddit', '_reset_attributes', '_safely_add_arguments', '_submission_class', '_submit_media', '_subreddit_collections_class', '_subreddit_list', '_upload_inline_media', '_upload_media', '_url_parts', '_validate_gallery', '_validate_inline_media', '_validate_time_filter', 'banned', 'collections', 'comments', 'contributor', 'controversial', 'display_name', 'emoji', 'filters', 'flair', 'fullname', 'gilded', 'hot', 'message', 'mod', 'moderator', 'modmail', 'muted', 'new', 'parse', 'post_requirements', 'quaran', 'random', 'random_rising', 'rising', 'rules', 'search', 'sticky', 'stream', 'stylesheet', 'submit', 'submit_gallery', 'submit_image', 'submit_poll', 'submit_video', 'subscribe', 'top', 'traffic', 'unsubscribe', 'widgets', 'wiki']\n"
     ]
    }
   ],
   "source": [
    "# Printing the methods and attributes associated with the subreddit instance\n",
    "print(dir(subreddit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_posts = subreddit.hot(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__next__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_exhausted', '_extract_sublist', '_list_index', '_listing', '_next_batch', '_reddit', '_safely_add_arguments', 'limit', 'params', 'parse', 'url', 'yielded']\n"
     ]
    }
   ],
   "source": [
    "print(dir(hot_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Sample Data\n",
    "* Using only 10 hot posts to get an idea of the data being generated.\n",
    "* After finalizing the features, data can be extracted from larger number of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly Discussion Megathread - June 12, 2022 (GMT+0)\n",
      "Celsius Withdrawals Stay Blocked for 5th Day as Clients Fear Losing All Funds\n",
      "He must be sad (((\n",
      "Elon Musk sued for $258 billion over dogecoin ‘pyramid scheme’\n",
      "Celsius Network has now paused withdrawals, swaps, and transfers for over 72 hours now.\n",
      "Circle to Launch Euro-pegged Stablecoin: Euro Coin\n",
      "Less than a Month after Launching, Over 100 Token Pools Deployed on Bancor v3\n",
      "Celsius can go bank themselves\n",
      "Yahoo is Launching Metaverse Events in Hong Kong\n",
      "MicroStrategy’s Michael Saylor On Bitcoin (BTC) Price: Now Is the “Ideal Entry Point” for Investors\n"
     ]
    }
   ],
   "source": [
    "sample_data = defaultdict(list)\n",
    "\n",
    "for post in hot_posts:\n",
    "    print(post.title)\n",
    "    sample_data['title'].append(post.title)\n",
    "    sample_data['score'].append(post.score)\n",
    "    sample_data['url'].append(post.url)\n",
    "    sample_data['num_comments'].append(post.num_comments)\n",
    "    sample_data['text'].append(post.selftext)\n",
    "    sample_data['created'].append(post.created)\n",
    "    sample_data['flair'].append(post.link_flair_text)\n",
    "    sample_data['original'].append(post.is_original_content)\n",
    "    sample_data['subreddit'].append(post.subreddit)\n",
    "    \n",
    "    post.comments.replace_more(limit=None)\n",
    "    all_comments = \"\"\n",
    "    for comment in post.comments:\n",
    "        all_comments += \" \" + comment.body\n",
    "#         print(\"Added: \", comment.body)\n",
    "    sample_data['comments'].append(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_df = pd.DataFrame(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "      <th>flair</th>\n",
       "      <th>original</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weekly Discussion Megathread - June 12, 2022 (...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>23</td>\n",
       "      <td>Welcome to the r/CryptoMarkets Weekly Discuss...</td>\n",
       "      <td>1.655039e+09</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>Today's Crypto Market status ：Extreme Fear! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Celsius Withdrawals Stay Blocked for 5th Day a...</td>\n",
       "      <td>91</td>\n",
       "      <td>https://tokenist.com/celsius-withdrawals-stay-...</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>1.655390e+09</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>In the FAQ email they just sent out, it says ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He must be sad (((</td>\n",
       "      <td>96</td>\n",
       "      <td>https://i.redd.it/08ibfyeafz591.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td>1.655385e+09</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>He’s buying the dip Look at your own portfoli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elon Musk sued for $258 billion over dogecoin ...</td>\n",
       "      <td>15</td>\n",
       "      <td>https://nypost.com/2022/06/16/elon-musk-sued-f...</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1.655403e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>That should prove interesting considering the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Celsius Network has now paused withdrawals, sw...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>4</td>\n",
       "      <td>Celsius Network has paused withdrawals, swaps,...</td>\n",
       "      <td>1.655393e+09</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>Without some takeovers or extreme limits, no....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  \\\n",
       "0  Weekly Discussion Megathread - June 12, 2022 (...      5   \n",
       "1  Celsius Withdrawals Stay Blocked for 5th Day a...     91   \n",
       "2                                 He must be sad (((     96   \n",
       "3  Elon Musk sued for $258 billion over dogecoin ...     15   \n",
       "4  Celsius Network has now paused withdrawals, sw...     13   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/CryptoMarkets/comment...            23   \n",
       "1  https://tokenist.com/celsius-withdrawals-stay-...            16   \n",
       "2                https://i.redd.it/08ibfyeafz591.jpg            13   \n",
       "3  https://nypost.com/2022/06/16/elon-musk-sued-f...             3   \n",
       "4  https://www.reddit.com/r/CryptoMarkets/comment...             4   \n",
       "\n",
       "                                                text       created  \\\n",
       "0   Welcome to the r/CryptoMarkets Weekly Discuss...  1.655039e+09   \n",
       "1                                                     1.655390e+09   \n",
       "2                                                     1.655385e+09   \n",
       "3                                                     1.655403e+09   \n",
       "4  Celsius Network has paused withdrawals, swaps,...  1.655393e+09   \n",
       "\n",
       "        flair  original      subreddit  \\\n",
       "0  Discussion     False  CryptoMarkets   \n",
       "1        NEWS     False  CryptoMarkets   \n",
       "2      COMEDY     False  CryptoMarkets   \n",
       "3        None     False  CryptoMarkets   \n",
       "4  DISCUSSION     False  CryptoMarkets   \n",
       "\n",
       "                                            comments  \n",
       "0   Today's Crypto Market status ：Extreme Fear! W...  \n",
       "1   In the FAQ email they just sent out, it says ...  \n",
       "2   He’s buying the dip Look at your own portfoli...  \n",
       "3   That should prove interesting considering the...  \n",
       "4   Without some takeovers or extreme limits, no....  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an additional feature of the polarity of the text using SIA\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "results = []\n",
    "\n",
    "for line in sample_df['comments']:\n",
    "    pol_score = sia.polarity_scores(line)\n",
    "    pol_score['comments'] = line\n",
    "    results.append(pol_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.6582</td>\n",
       "      <td>Today's Crypto Market status ：Extreme Fear! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.173</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>In the FAQ email they just sent out, it says ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.116</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.093</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>He’s buying the dip Look at your own portfoli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.042</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>That should prove interesting considering the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.6265</td>\n",
       "      <td>Without some takeovers or extreme limits, no....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound  \\\n",
       "0  0.100  0.822  0.078   -0.6582   \n",
       "1  0.173  0.687  0.140   -0.7482   \n",
       "2  0.116  0.792  0.093   -0.6597   \n",
       "3  0.042  0.890  0.068    0.1027   \n",
       "4  0.088  0.912  0.000   -0.6265   \n",
       "\n",
       "                                            comments  \n",
       "0   Today's Crypto Market status ：Extreme Fear! W...  \n",
       "1   In the FAQ email they just sent out, it says ...  \n",
       "2   He’s buying the dip Look at your own portfoli...  \n",
       "3   That should prove interesting considering the...  \n",
       "4   Without some takeovers or extreme limits, no....  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity = df2['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = sample_df.join(polarity)\n",
    "sample_df.rename(columns = {'compound':'polarity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "      <th>flair</th>\n",
       "      <th>original</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weekly Discussion Megathread - June 12, 2022 (...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>23</td>\n",
       "      <td>Welcome to the r/CryptoMarkets Weekly Discuss...</td>\n",
       "      <td>1.655039e+09</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>Today's Crypto Market status ：Extreme Fear! W...</td>\n",
       "      <td>-0.6582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Celsius Withdrawals Stay Blocked for 5th Day a...</td>\n",
       "      <td>91</td>\n",
       "      <td>https://tokenist.com/celsius-withdrawals-stay-...</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td>1.655390e+09</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>In the FAQ email they just sent out, it says ...</td>\n",
       "      <td>-0.7482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>He must be sad (((</td>\n",
       "      <td>96</td>\n",
       "      <td>https://i.redd.it/08ibfyeafz591.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td>1.655385e+09</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>He’s buying the dip Look at your own portfoli...</td>\n",
       "      <td>-0.6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elon Musk sued for $258 billion over dogecoin ...</td>\n",
       "      <td>15</td>\n",
       "      <td>https://nypost.com/2022/06/16/elon-musk-sued-f...</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1.655403e+09</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>That should prove interesting considering the...</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Celsius Network has now paused withdrawals, sw...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>4</td>\n",
       "      <td>Celsius Network has paused withdrawals, swaps,...</td>\n",
       "      <td>1.655393e+09</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>Without some takeovers or extreme limits, no....</td>\n",
       "      <td>-0.6265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  \\\n",
       "0  Weekly Discussion Megathread - June 12, 2022 (...      5   \n",
       "1  Celsius Withdrawals Stay Blocked for 5th Day a...     91   \n",
       "2                                 He must be sad (((     96   \n",
       "3  Elon Musk sued for $258 billion over dogecoin ...     15   \n",
       "4  Celsius Network has now paused withdrawals, sw...     13   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/CryptoMarkets/comment...            23   \n",
       "1  https://tokenist.com/celsius-withdrawals-stay-...            16   \n",
       "2                https://i.redd.it/08ibfyeafz591.jpg            13   \n",
       "3  https://nypost.com/2022/06/16/elon-musk-sued-f...             3   \n",
       "4  https://www.reddit.com/r/CryptoMarkets/comment...             4   \n",
       "\n",
       "                                                text       created  \\\n",
       "0   Welcome to the r/CryptoMarkets Weekly Discuss...  1.655039e+09   \n",
       "1                                                     1.655390e+09   \n",
       "2                                                     1.655385e+09   \n",
       "3                                                     1.655403e+09   \n",
       "4  Celsius Network has paused withdrawals, swaps,...  1.655393e+09   \n",
       "\n",
       "        flair  original      subreddit  \\\n",
       "0  Discussion     False  CryptoMarkets   \n",
       "1        NEWS     False  CryptoMarkets   \n",
       "2      COMEDY     False  CryptoMarkets   \n",
       "3        None     False  CryptoMarkets   \n",
       "4  DISCUSSION     False  CryptoMarkets   \n",
       "\n",
       "                                            comments  polarity  \n",
       "0   Today's Crypto Market status ：Extreme Fear! W...   -0.6582  \n",
       "1   In the FAQ email they just sent out, it says ...   -0.7482  \n",
       "2   He’s buying the dip Look at your own portfoli...   -0.6597  \n",
       "3   That should prove interesting considering the...    0.1027  \n",
       "4   Without some takeovers or extreme limits, no....   -0.6265  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length of comment:  2992\n",
      "Min Length of comment:  0\n",
      "Average Length of comment:  673.5\n"
     ]
    }
   ],
   "source": [
    "# Info about the length of the comments\n",
    "total_posts = len(sample_df['comments'])\n",
    "total_posts\n",
    "\n",
    "inf = np.inf\n",
    "max_len_comments = -inf\n",
    "min_len_comments = inf\n",
    "avg_len_comments = 0;\n",
    "\n",
    "total_len_comments = 0\n",
    "for comments in sample_df['comments']:\n",
    "    total_len_comments += len(comments)\n",
    "    max_len_comments = max(max_len_comments, len(comments))\n",
    "    min_len_comments = min(min_len_comments, len(comments))\n",
    "\n",
    "avg_len_comments = total_len_comments/total_posts\n",
    "\n",
    "print(\"Max Length of comment: \", max_len_comments)\n",
    "print(\"Min Length of comment: \", min_len_comments)\n",
    "print(\"Average Length of comment: \", avg_len_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime as dt\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['timestamp'] = df_copy['created'].apply(get_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    # remove stock market tickers like $GE\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    \n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    text = re.sub(r'#', '', text)\n",
    "    \n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    text_clean = []\n",
    "    for word in text_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            text_clean.append(stem_word)\n",
    "\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['title'] = df_copy['title'].apply(preprocess_text)\n",
    "df_copy['comments'] = df_copy['comments'].apply(preprocess_text)\n",
    "df_copy['text'] = df_copy['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aryan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.drop(['created'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>text</th>\n",
       "      <th>flair</th>\n",
       "      <th>original</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[weekli, discuss, megathread, june, 12, 2022, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>23</td>\n",
       "      <td>[welcom, r/cryptomarket, weekli, discuss, thre...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[today, 's, crypto, market, statu, ：extrem, fe...</td>\n",
       "      <td>-0.6582</td>\n",
       "      <td>2022-06-12 18:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[celsiu, withdraw, stay, block, 5th, day, clie...</td>\n",
       "      <td>91</td>\n",
       "      <td>https://tokenist.com/celsius-withdrawals-stay-...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[in, faq, email, sent, say, investor, continu,...</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>2022-06-16 19:59:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[he, must, sad]</td>\n",
       "      <td>96</td>\n",
       "      <td>https://i.redd.it/08ibfyeafz591.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[he, ’, buy, dip, look, portfolio, 😁, if, beli...</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>2022-06-16 18:36:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[elon, musk, su, billion, dogecoin, ‘, pyramid...</td>\n",
       "      <td>15</td>\n",
       "      <td>https://nypost.com/2022/06/16/elon-musk-sued-f...</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[that, prove, interest, consid, take, suit, it...</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>2022-06-16 23:31:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[celsiu, network, paus, withdraw, swap, transf...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>4</td>\n",
       "      <td>[celsiu, network, paus, withdraw, swap, transf...</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[without, takeov, extrem, limit, you, n't, bri...</td>\n",
       "      <td>-0.6265</td>\n",
       "      <td>2022-06-16 20:51:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  \\\n",
       "0  [weekli, discuss, megathread, june, 12, 2022, ...      5   \n",
       "1  [celsiu, withdraw, stay, block, 5th, day, clie...     91   \n",
       "2                                    [he, must, sad]     96   \n",
       "3  [elon, musk, su, billion, dogecoin, ‘, pyramid...     15   \n",
       "4  [celsiu, network, paus, withdraw, swap, transf...     13   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/CryptoMarkets/comment...            23   \n",
       "1  https://tokenist.com/celsius-withdrawals-stay-...            16   \n",
       "2                https://i.redd.it/08ibfyeafz591.jpg            13   \n",
       "3  https://nypost.com/2022/06/16/elon-musk-sued-f...             3   \n",
       "4  https://www.reddit.com/r/CryptoMarkets/comment...             4   \n",
       "\n",
       "                                                text       flair  original  \\\n",
       "0  [welcom, r/cryptomarket, weekli, discuss, thre...  Discussion     False   \n",
       "1                                                 []        NEWS     False   \n",
       "2                                                 []      COMEDY     False   \n",
       "3                                                 []        None     False   \n",
       "4  [celsiu, network, paus, withdraw, swap, transf...  DISCUSSION     False   \n",
       "\n",
       "       subreddit                                           comments  polarity  \\\n",
       "0  CryptoMarkets  [today, 's, crypto, market, statu, ：extrem, fe...   -0.6582   \n",
       "1  CryptoMarkets  [in, faq, email, sent, say, investor, continu,...   -0.7482   \n",
       "2  CryptoMarkets  [he, ’, buy, dip, look, portfolio, 😁, if, beli...   -0.6597   \n",
       "3  CryptoMarkets  [that, prove, interest, consid, take, suit, it...    0.1027   \n",
       "4  CryptoMarkets  [without, takeov, extrem, limit, you, n't, bri...   -0.6265   \n",
       "\n",
       "            timestamp  \n",
       "0 2022-06-12 18:30:10  \n",
       "1 2022-06-16 19:59:29  \n",
       "2 2022-06-16 18:36:18  \n",
       "3 2022-06-16 23:31:44  \n",
       "4 2022-06-16 20:51:04  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the data to a csv file\n",
    "df_copy.to_csv('praw_sample_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the flairs\n",
    "Picking only those flairs that appeared atleast 5 times in the subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>text</th>\n",
       "      <th>flair</th>\n",
       "      <th>original</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['weekli', 'discuss', 'megathread', 'june', '1...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>23</td>\n",
       "      <td>['welcom', 'r/cryptomarket', 'weekli', 'discus...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['today', \"'s\", 'crypto', 'market', 'statu', '...</td>\n",
       "      <td>-0.6582</td>\n",
       "      <td>2022-06-12 18:30:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['celsiu', 'withdraw', 'stay', 'block', '5th',...</td>\n",
       "      <td>91</td>\n",
       "      <td>https://tokenist.com/celsius-withdrawals-stay-...</td>\n",
       "      <td>16</td>\n",
       "      <td>[]</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['in', 'faq', 'email', 'sent', 'say', 'investo...</td>\n",
       "      <td>-0.7482</td>\n",
       "      <td>2022-06-16 19:59:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['he', 'must', 'sad']</td>\n",
       "      <td>96</td>\n",
       "      <td>https://i.redd.it/08ibfyeafz591.jpg</td>\n",
       "      <td>13</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['he', '’', 'buy', 'dip', 'look', 'portfolio',...</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>2022-06-16 18:36:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['elon', 'musk', 'su', 'billion', 'dogecoin', ...</td>\n",
       "      <td>15</td>\n",
       "      <td>https://nypost.com/2022/06/16/elon-musk-sued-f...</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['that', 'prove', 'interest', 'consid', 'take'...</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>2022-06-16 23:31:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['celsiu', 'network', 'paus', 'withdraw', 'swa...</td>\n",
       "      <td>13</td>\n",
       "      <td>https://www.reddit.com/r/CryptoMarkets/comment...</td>\n",
       "      <td>4</td>\n",
       "      <td>['celsiu', 'network', 'paus', 'withdraw', 'swa...</td>\n",
       "      <td>DISCUSSION</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['without', 'takeov', 'extrem', 'limit', 'you'...</td>\n",
       "      <td>-0.6265</td>\n",
       "      <td>2022-06-16 20:51:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['circl', 'launch', 'euro-peg', 'stablecoin', ...</td>\n",
       "      <td>12</td>\n",
       "      <td>https://coincodecap.com/circle-to-launch-euro-...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2022-06-16 22:01:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['less', 'month', 'launch', 'over', '100', 'to...</td>\n",
       "      <td>18</td>\n",
       "      <td>https://zycrypto.com/less-than-a-month-after-l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['bancor', 'one', 'project', 'take', 'real', '...</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>2022-06-16 18:21:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['celsiu', 'go', 'bank']</td>\n",
       "      <td>7</td>\n",
       "      <td>https://youtube.com/shorts/zkCsNlOvDFk?feature...</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['“', 'let', '’', 'put', 'alt', 'coin', 'aaaaa...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2022-06-16 20:15:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['yahoo', 'launch', 'metavers', 'event', 'hong...</td>\n",
       "      <td>5</td>\n",
       "      <td>https://coincodecap.com/yahoo-is-launching-met...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2022-06-16 21:32:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['microstrategi', '’', 'michael', 'saylor', 'o...</td>\n",
       "      <td>136</td>\n",
       "      <td>https://timestabloid.com/microstrategys-michae...</td>\n",
       "      <td>62</td>\n",
       "      <td>[]</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>False</td>\n",
       "      <td>CryptoMarkets</td>\n",
       "      <td>['bitcoin', 'pro', '/r/cryptomarkets/comments/...</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>2022-06-16 02:12:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  score  \\\n",
       "0  ['weekli', 'discuss', 'megathread', 'june', '1...      5   \n",
       "1  ['celsiu', 'withdraw', 'stay', 'block', '5th',...     91   \n",
       "2                              ['he', 'must', 'sad']     96   \n",
       "3  ['elon', 'musk', 'su', 'billion', 'dogecoin', ...     15   \n",
       "4  ['celsiu', 'network', 'paus', 'withdraw', 'swa...     13   \n",
       "5  ['circl', 'launch', 'euro-peg', 'stablecoin', ...     12   \n",
       "6  ['less', 'month', 'launch', 'over', '100', 'to...     18   \n",
       "7                           ['celsiu', 'go', 'bank']      7   \n",
       "8  ['yahoo', 'launch', 'metavers', 'event', 'hong...      5   \n",
       "9  ['microstrategi', '’', 'michael', 'saylor', 'o...    136   \n",
       "\n",
       "                                                 url  num_comments  \\\n",
       "0  https://www.reddit.com/r/CryptoMarkets/comment...            23   \n",
       "1  https://tokenist.com/celsius-withdrawals-stay-...            16   \n",
       "2                https://i.redd.it/08ibfyeafz591.jpg            13   \n",
       "3  https://nypost.com/2022/06/16/elon-musk-sued-f...             3   \n",
       "4  https://www.reddit.com/r/CryptoMarkets/comment...             4   \n",
       "5  https://coincodecap.com/circle-to-launch-euro-...             0   \n",
       "6  https://zycrypto.com/less-than-a-month-after-l...             1   \n",
       "7  https://youtube.com/shorts/zkCsNlOvDFk?feature...             2   \n",
       "8  https://coincodecap.com/yahoo-is-launching-met...             0   \n",
       "9  https://timestabloid.com/microstrategys-michae...            62   \n",
       "\n",
       "                                                text       flair  original  \\\n",
       "0  ['welcom', 'r/cryptomarket', 'weekli', 'discus...  Discussion     False   \n",
       "1                                                 []        NEWS     False   \n",
       "2                                                 []      COMEDY     False   \n",
       "3                                                 []         NaN     False   \n",
       "4  ['celsiu', 'network', 'paus', 'withdraw', 'swa...  DISCUSSION     False   \n",
       "5                                                 []         NaN     False   \n",
       "6                                                 []         NaN     False   \n",
       "7                                                 []         NaN     False   \n",
       "8                                                 []         NaN     False   \n",
       "9                                                 []        NEWS     False   \n",
       "\n",
       "       subreddit                                           comments  polarity  \\\n",
       "0  CryptoMarkets  ['today', \"'s\", 'crypto', 'market', 'statu', '...   -0.6582   \n",
       "1  CryptoMarkets  ['in', 'faq', 'email', 'sent', 'say', 'investo...   -0.7482   \n",
       "2  CryptoMarkets  ['he', '’', 'buy', 'dip', 'look', 'portfolio',...   -0.6597   \n",
       "3  CryptoMarkets  ['that', 'prove', 'interest', 'consid', 'take'...    0.1027   \n",
       "4  CryptoMarkets  ['without', 'takeov', 'extrem', 'limit', 'you'...   -0.6265   \n",
       "5  CryptoMarkets                                                 []    0.0000   \n",
       "6  CryptoMarkets  ['bancor', 'one', 'project', 'take', 'real', '...   -0.0258   \n",
       "7  CryptoMarkets  ['“', 'let', '’', 'put', 'alt', 'coin', 'aaaaa...    0.0000   \n",
       "8  CryptoMarkets                                                 []    0.0000   \n",
       "9  CryptoMarkets  ['bitcoin', 'pro', '/r/cryptomarkets/comments/...    0.9720   \n",
       "\n",
       "             timestamp  \n",
       "0  2022-06-12 18:30:10  \n",
       "1  2022-06-16 19:59:29  \n",
       "2  2022-06-16 18:36:18  \n",
       "3  2022-06-16 23:31:44  \n",
       "4  2022-06-16 20:51:04  \n",
       "5  2022-06-16 22:01:47  \n",
       "6  2022-06-16 18:21:01  \n",
       "7  2022-06-16 20:15:49  \n",
       "8  2022-06-16 21:32:02  \n",
       "9  2022-06-16 02:12:07  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('praw_sample_data.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.800000</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>-0.164370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>48.435065</td>\n",
       "      <td>19.120669</td>\n",
       "      <td>0.527737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.748200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.250000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>-0.650275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>-0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72.750000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>136.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.972000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score  num_comments   polarity\n",
       "count   10.000000     10.000000  10.000000\n",
       "mean    39.800000     12.400000  -0.164370\n",
       "std     48.435065     19.120669   0.527737\n",
       "min      5.000000      0.000000  -0.748200\n",
       "25%      8.250000      1.250000  -0.650275\n",
       "50%     14.000000      3.500000  -0.012900\n",
       "75%     72.750000     15.250000   0.000000\n",
       "max    136.000000     62.000000   0.972000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title           0\n",
       "score           0\n",
       "url             0\n",
       "num_comments    0\n",
       "text            0\n",
       "flair           5\n",
       "original        0\n",
       "subreddit       0\n",
       "comments        0\n",
       "polarity        0\n",
       "timestamp       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "flairs = []\n",
    "\n",
    "for post in reddit.subreddit('cryptomarkets').top(time_filter=\"all\", limit=500):\n",
    "    flairs.append(post.link_flair_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flairs = pd.DataFrame(flairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMEDY                        118\n",
       "NEWS                          108\n",
       "Comedy                         27\n",
       "DISCUSSION                     26\n",
       "News                           15\n",
       "SENTIMENT                      14\n",
       "FUNDAMENTALS                   10\n",
       "Technical Analysis              9\n",
       "TECHNICALS                      8\n",
       "STRATEGY                        7\n",
       "Educational                     6\n",
       "Strategy                        5\n",
       "Exchange                        5\n",
       "Sentiment                       4\n",
       "NEW COIN                        4\n",
       "EXCHANGE                        3\n",
       "Tool                            3\n",
       "WARNING                         2\n",
       "ANALYSIS                        2\n",
       "Announcement                    2\n",
       "Warning                         2\n",
       "History                         1\n",
       "Support-Open                    1\n",
       "Fundmental Analysis             1\n",
       "TOOL                            1\n",
       "Fundamentals                    1\n",
       "Discussion                      1\n",
       "UNCONFIRMED                     1\n",
       "Best Daily Meme - 09/23/19      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flairs.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_count = {}\n",
    "\n",
    "for i in flairs:\n",
    "    if i in flair_count.keys():\n",
    "        flair_count[i] += 1\n",
    "    else:\n",
    "        flair_count[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['News', 'FUNDAMENTALS', 'NEWS', 'DISCUSSION', 'TECHNICALS', 'Comedy', 'COMEDY', 'Exchange', 'SENTIMENT', 'STRATEGY', 'Strategy', 'Technical Analysis', 'Educational']\n"
     ]
    }
   ],
   "source": [
    "final_flairs = []\n",
    "\n",
    "for flair_unique in set(flairs):\n",
    "    if(flair_count[flair_unique]>4):\n",
    "        final_flairs.append(flair_unique)\n",
    "\n",
    "final_flairs.remove(None)\n",
    "        \n",
    "print(final_flairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'News',\n",
      " 1: 'FUNDAMENTALS',\n",
      " 2: 'NEWS',\n",
      " 3: 'DISCUSSION',\n",
      " 4: 'TECHNICALS',\n",
      " 5: 'Comedy',\n",
      " 6: 'COMEDY',\n",
      " 7: 'Exchange',\n",
      " 8: 'SENTIMENT',\n",
      " 9: 'STRATEGY',\n",
      " 10: 'Strategy',\n",
      " 11: 'Technical Analysis',\n",
      " 12: 'Educational'}\n"
     ]
    }
   ],
   "source": [
    "# Converting the flairs to a numeric format\n",
    "flair_values = {}\n",
    "\n",
    "for i in range(len(final_flairs)):\n",
    "    flair_values[i] = final_flairs[i]\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(flair_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  1 samples\n",
      "Added  2 samples\n",
      "Added  3 samples\n",
      "Added  4 samples\n",
      "Added  5 samples\n",
      "Added  6 samples\n",
      "Added  7 samples\n",
      "Added  8 samples\n",
      "Added  9 samples\n",
      "Added  10 samples\n",
      "Added  11 samples\n",
      "Added  12 samples\n",
      "Added  13 samples\n",
      "Added  14 samples\n",
      "Added  15 samples\n",
      "Added  16 samples\n",
      "Added  17 samples\n",
      "Added  18 samples\n",
      "Added  19 samples\n",
      "Added  20 samples\n",
      "Added  21 samples\n",
      "Added  22 samples\n",
      "Added  23 samples\n",
      "Added  24 samples\n",
      "Added  25 samples\n",
      "Added  26 samples\n",
      "Added  27 samples\n",
      "Added  28 samples\n",
      "Added  29 samples\n",
      "Added  30 samples\n",
      "Added  31 samples\n",
      "Added  32 samples\n",
      "Added  33 samples\n",
      "Added  34 samples\n",
      "Added  35 samples\n",
      "Added  36 samples\n",
      "Added  37 samples\n",
      "Added  38 samples\n",
      "Added  39 samples\n",
      "Added  40 samples\n",
      "Added  41 samples\n",
      "Added  42 samples\n",
      "Added  43 samples\n",
      "Added  44 samples\n",
      "Added  45 samples\n",
      "Added  46 samples\n",
      "Added  47 samples\n",
      "Added  48 samples\n",
      "Added  49 samples\n",
      "Added  50 samples\n",
      "Added  51 samples\n",
      "Added  52 samples\n",
      "Added  53 samples\n",
      "Added  54 samples\n",
      "Added  55 samples\n",
      "Added  56 samples\n",
      "Added  57 samples\n",
      "Added  58 samples\n",
      "Added  59 samples\n",
      "Added  60 samples\n",
      "Added  61 samples\n",
      "Added  62 samples\n",
      "Added  63 samples\n",
      "Added  64 samples\n",
      "Added  65 samples\n",
      "Added  66 samples\n",
      "Added  67 samples\n",
      "Added  68 samples\n",
      "Added  69 samples\n",
      "Added  70 samples\n",
      "Added  71 samples\n",
      "Added  72 samples\n",
      "Added  73 samples\n",
      "Added  74 samples\n",
      "Added  75 samples\n",
      "Added  76 samples\n",
      "Added  77 samples\n",
      "Added  78 samples\n",
      "Added  79 samples\n",
      "Added  80 samples\n",
      "Added  81 samples\n",
      "Added  82 samples\n",
      "Added  83 samples\n",
      "Added  84 samples\n",
      "Added  85 samples\n",
      "Added  86 samples\n",
      "Added  87 samples\n",
      "Added  88 samples\n",
      "Added  89 samples\n",
      "Added  90 samples\n",
      "Added  91 samples\n",
      "Added  92 samples\n",
      "Added  93 samples\n",
      "Added  94 samples\n",
      "Added  95 samples\n",
      "Added  96 samples\n",
      "Added  97 samples\n",
      "Added  98 samples\n",
      "Added  99 samples\n",
      "Added  100 samples\n",
      "Added  101 samples\n",
      "Added  102 samples\n",
      "Added  103 samples\n",
      "Added  104 samples\n",
      "Added  105 samples\n",
      "Added  106 samples\n",
      "Added  107 samples\n",
      "Added  108 samples\n",
      "Added  109 samples\n",
      "Added  110 samples\n",
      "Added  111 samples\n",
      "Added  112 samples\n",
      "Added  113 samples\n",
      "Added  114 samples\n",
      "Added  115 samples\n",
      "Added  116 samples\n",
      "Added  117 samples\n",
      "Added  118 samples\n",
      "Added  119 samples\n",
      "Added  120 samples\n",
      "Added  121 samples\n",
      "Added  122 samples\n",
      "Added  123 samples\n",
      "Added  124 samples\n",
      "Added  125 samples\n",
      "Added  126 samples\n",
      "Added  127 samples\n",
      "Added  128 samples\n",
      "Added  129 samples\n",
      "Added  130 samples\n",
      "Added  131 samples\n",
      "Added  132 samples\n",
      "Added  133 samples\n",
      "Added  134 samples\n",
      "Added  135 samples\n",
      "Added  136 samples\n",
      "Added  137 samples\n",
      "Added  138 samples\n",
      "Added  139 samples\n",
      "Added  140 samples\n",
      "Added  141 samples\n",
      "Added  142 samples\n",
      "Added  143 samples\n",
      "Added  144 samples\n",
      "Added  145 samples\n",
      "Added  146 samples\n",
      "Added  147 samples\n",
      "Added  148 samples\n",
      "Added  149 samples\n",
      "Added  150 samples\n",
      "Added  151 samples\n",
      "Added  152 samples\n",
      "Added  153 samples\n",
      "Added  154 samples\n",
      "Added  155 samples\n",
      "Added  156 samples\n",
      "Added  157 samples\n",
      "Added  158 samples\n",
      "Added  159 samples\n",
      "Added  160 samples\n",
      "Added  161 samples\n",
      "Added  162 samples\n",
      "Added  163 samples\n",
      "Added  164 samples\n",
      "Added  165 samples\n",
      "Added  166 samples\n",
      "Added  167 samples\n",
      "Added  168 samples\n",
      "Added  169 samples\n",
      "Added  170 samples\n",
      "Added  171 samples\n",
      "Added  172 samples\n",
      "Added  173 samples\n",
      "Added  174 samples\n",
      "Added  175 samples\n",
      "Added  176 samples\n",
      "Added  177 samples\n",
      "Added  178 samples\n",
      "Added  179 samples\n",
      "Added  180 samples\n",
      "Added  181 samples\n",
      "Added  182 samples\n",
      "Added  183 samples\n",
      "Added  184 samples\n",
      "Added  185 samples\n",
      "Added  186 samples\n",
      "Added  187 samples\n",
      "Added  188 samples\n",
      "Added  189 samples\n",
      "Added  190 samples\n",
      "Added  191 samples\n",
      "Added  192 samples\n",
      "Added  193 samples\n",
      "Added  194 samples\n",
      "Added  195 samples\n",
      "Added  196 samples\n",
      "Added  197 samples\n",
      "Added  198 samples\n",
      "Added  199 samples\n",
      "Added  200 samples\n",
      "Added  201 samples\n",
      "Added  202 samples\n",
      "Added  203 samples\n",
      "Added  204 samples\n",
      "Added  205 samples\n",
      "Added  206 samples\n",
      "Added  207 samples\n",
      "Added  208 samples\n",
      "Added  209 samples\n",
      "Added  210 samples\n",
      "Added  211 samples\n",
      "Added  212 samples\n",
      "Added  213 samples\n",
      "Added  214 samples\n",
      "Added  215 samples\n",
      "Added  216 samples\n",
      "Added  217 samples\n",
      "Added  218 samples\n",
      "Added  219 samples\n",
      "Added  220 samples\n",
      "Added  221 samples\n",
      "Added  222 samples\n",
      "Added  223 samples\n",
      "Added  224 samples\n",
      "Added  225 samples\n",
      "Added  226 samples\n",
      "Added  227 samples\n",
      "Added  228 samples\n",
      "Added  229 samples\n",
      "Added  230 samples\n",
      "Added  231 samples\n",
      "Added  232 samples\n",
      "Added  233 samples\n",
      "Added  234 samples\n",
      "Added  235 samples\n",
      "Added  236 samples\n",
      "Added  237 samples\n",
      "Added  238 samples\n",
      "Added  239 samples\n",
      "Added  240 samples\n",
      "Added  241 samples\n",
      "Added  242 samples\n",
      "Added  243 samples\n",
      "Added  244 samples\n",
      "Added  245 samples\n",
      "Added  246 samples\n",
      "Added  247 samples\n",
      "Added  248 samples\n",
      "Added  249 samples\n",
      "Added  250 samples\n",
      "Added  251 samples\n",
      "Added  252 samples\n",
      "Added  253 samples\n",
      "Added  254 samples\n",
      "Added  255 samples\n",
      "Added  256 samples\n",
      "Added  257 samples\n",
      "Added  258 samples\n",
      "Added  259 samples\n",
      "Added  260 samples\n",
      "Added  261 samples\n",
      "Added  262 samples\n",
      "Added  263 samples\n",
      "Added  264 samples\n",
      "Added  265 samples\n",
      "Added  266 samples\n",
      "Added  267 samples\n",
      "Added  268 samples\n",
      "Added  269 samples\n",
      "Added  270 samples\n",
      "Added  271 samples\n",
      "Added  272 samples\n",
      "Added  273 samples\n",
      "Added  274 samples\n",
      "Added  275 samples\n",
      "Added  276 samples\n",
      "Added  277 samples\n",
      "Added  278 samples\n",
      "Added  279 samples\n",
      "Added  280 samples\n",
      "Added  281 samples\n",
      "Added  282 samples\n",
      "Added  283 samples\n",
      "Added  284 samples\n",
      "Added  285 samples\n",
      "Added  286 samples\n",
      "Added  287 samples\n",
      "Added  288 samples\n",
      "Added  289 samples\n",
      "Added  290 samples\n",
      "Added  291 samples\n",
      "Added  292 samples\n",
      "Added  293 samples\n",
      "Added  294 samples\n",
      "Added  295 samples\n",
      "Added  296 samples\n",
      "Added  297 samples\n",
      "Added  298 samples\n",
      "Added  299 samples\n",
      "Added  300 samples\n",
      "Added  301 samples\n",
      "Added  302 samples\n",
      "Added  303 samples\n",
      "Added  304 samples\n",
      "Added  305 samples\n",
      "Added  306 samples\n",
      "Added  307 samples\n",
      "Added  308 samples\n",
      "Added  309 samples\n",
      "Added  310 samples\n",
      "Added  311 samples\n",
      "Added  312 samples\n",
      "Added  313 samples\n",
      "Added  314 samples\n",
      "Added  315 samples\n",
      "Added  316 samples\n",
      "Added  317 samples\n",
      "Added  318 samples\n",
      "Added  319 samples\n",
      "Added  320 samples\n",
      "Added  321 samples\n",
      "Added  322 samples\n",
      "Added  323 samples\n",
      "Added  324 samples\n",
      "Added  325 samples\n",
      "Added  326 samples\n",
      "Added  327 samples\n",
      "Added  328 samples\n",
      "Added  329 samples\n",
      "Added  330 samples\n",
      "Added  331 samples\n",
      "Added  332 samples\n",
      "Added  333 samples\n",
      "Added  334 samples\n",
      "Added  335 samples\n",
      "Added  336 samples\n",
      "Added  337 samples\n",
      "Added  338 samples\n",
      "Added  339 samples\n",
      "Added  340 samples\n",
      "Added  341 samples\n",
      "Added  342 samples\n",
      "Added  343 samples\n",
      "Added  344 samples\n",
      "Added  345 samples\n",
      "Added  346 samples\n",
      "Added  347 samples\n",
      "Added  348 samples\n",
      "Added  349 samples\n",
      "Added  350 samples\n",
      "Added  351 samples\n",
      "Added  352 samples\n",
      "Added  353 samples\n",
      "Added  354 samples\n",
      "Added  355 samples\n",
      "Added  356 samples\n",
      "Added  357 samples\n",
      "Added  358 samples\n",
      "Added  359 samples\n",
      "Added  360 samples\n",
      "Added  361 samples\n",
      "Added  362 samples\n",
      "Added  363 samples\n",
      "Added  364 samples\n",
      "Added  365 samples\n",
      "Added  366 samples\n",
      "Added  367 samples\n",
      "Added  368 samples\n",
      "Added  369 samples\n",
      "Added  370 samples\n",
      "Added  371 samples\n",
      "Added  372 samples\n",
      "Added  373 samples\n",
      "Added  374 samples\n",
      "Added  375 samples\n",
      "Added  376 samples\n",
      "Added  377 samples\n",
      "Added  378 samples\n",
      "Added  379 samples\n",
      "Added  380 samples\n",
      "Added  381 samples\n",
      "Added  382 samples\n",
      "Added  383 samples\n",
      "Added  384 samples\n",
      "Added  385 samples\n",
      "Added  386 samples\n",
      "Added  387 samples\n",
      "Added  388 samples\n",
      "Added  389 samples\n",
      "Added  390 samples\n",
      "Added  391 samples\n",
      "Added  392 samples\n",
      "Added  393 samples\n",
      "Added  394 samples\n",
      "Added  395 samples\n",
      "Added  396 samples\n",
      "Added  397 samples\n",
      "Added  398 samples\n",
      "Added  399 samples\n",
      "Added  400 samples\n",
      "Added  401 samples\n",
      "Added  402 samples\n",
      "Added  403 samples\n",
      "Added  404 samples\n",
      "Added  405 samples\n",
      "Added  406 samples\n",
      "Added  407 samples\n",
      "Added  408 samples\n",
      "Added  409 samples\n",
      "Added  410 samples\n",
      "Added  411 samples\n",
      "Added  412 samples\n",
      "Added  413 samples\n",
      "Added  414 samples\n",
      "Added  415 samples\n",
      "Added  416 samples\n",
      "Added  417 samples\n",
      "Added  418 samples\n",
      "Added  419 samples\n",
      "Added  420 samples\n",
      "Added  421 samples\n",
      "Added  422 samples\n",
      "Added  423 samples\n",
      "Added  424 samples\n",
      "Added  425 samples\n",
      "Added  426 samples\n",
      "Added  427 samples\n",
      "Added  428 samples\n",
      "Added  429 samples\n",
      "Added  430 samples\n",
      "Added  431 samples\n",
      "Added  432 samples\n",
      "Added  433 samples\n",
      "Added  434 samples\n",
      "Added  435 samples\n",
      "Added  436 samples\n",
      "Added  437 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  438 samples\n",
      "Added  439 samples\n",
      "Added  440 samples\n",
      "Added  441 samples\n",
      "Added  442 samples\n",
      "Added  443 samples\n",
      "Added  444 samples\n",
      "Added  445 samples\n",
      "Added  446 samples\n",
      "Added  447 samples\n",
      "Added  448 samples\n",
      "Added  449 samples\n",
      "Added  450 samples\n",
      "Added  451 samples\n",
      "Added  452 samples\n",
      "Added  453 samples\n",
      "Added  454 samples\n",
      "Added  455 samples\n",
      "Added  456 samples\n",
      "Added  457 samples\n",
      "Added  458 samples\n",
      "Added  459 samples\n",
      "Added  460 samples\n",
      "Added  461 samples\n",
      "Added  462 samples\n",
      "Added  463 samples\n",
      "Added  464 samples\n",
      "Added  465 samples\n",
      "Added  466 samples\n",
      "Added  467 samples\n",
      "Added  468 samples\n",
      "Added  469 samples\n",
      "Added  470 samples\n",
      "Added  471 samples\n",
      "Added  472 samples\n",
      "Added  473 samples\n",
      "Added  474 samples\n",
      "Added  475 samples\n",
      "Added  476 samples\n",
      "Added  477 samples\n",
      "Added  478 samples\n",
      "Added  479 samples\n",
      "Added  480 samples\n",
      "Added  481 samples\n",
      "Added  482 samples\n",
      "Added  483 samples\n",
      "Added  484 samples\n",
      "Added  485 samples\n",
      "Added  486 samples\n",
      "Added  487 samples\n",
      "Added  488 samples\n",
      "Added  489 samples\n",
      "Added  490 samples\n",
      "Added  491 samples\n",
      "Added  492 samples\n",
      "Added  493 samples\n",
      "Added  494 samples\n",
      "Added  495 samples\n",
      "Added  496 samples\n",
      "Added  497 samples\n",
      "Added  498 samples\n",
      "Added  499 samples\n",
      "Added  500 samples\n",
      "Added  501 samples\n",
      "Added  502 samples\n",
      "Added  503 samples\n",
      "Added  504 samples\n",
      "Added  505 samples\n",
      "Added  506 samples\n",
      "Added  507 samples\n",
      "Added  508 samples\n",
      "Added  509 samples\n",
      "Added  510 samples\n",
      "Added  511 samples\n",
      "Added  512 samples\n",
      "Added  513 samples\n",
      "Added  514 samples\n",
      "Added  515 samples\n",
      "Added  516 samples\n",
      "Added  517 samples\n",
      "Added  518 samples\n",
      "Added  519 samples\n",
      "Added  520 samples\n",
      "Added  521 samples\n",
      "Added  522 samples\n",
      "Added  523 samples\n",
      "Added  524 samples\n",
      "Added  525 samples\n",
      "Added  526 samples\n",
      "Added  527 samples\n",
      "Added  528 samples\n",
      "Added  529 samples\n",
      "Added  530 samples\n",
      "Added  531 samples\n",
      "Added  532 samples\n",
      "Added  533 samples\n",
      "Added  534 samples\n",
      "Added  535 samples\n",
      "Added  536 samples\n",
      "Added  537 samples\n",
      "Added  538 samples\n",
      "Added  539 samples\n",
      "Added  540 samples\n",
      "Added  541 samples\n",
      "Added  542 samples\n",
      "Added  543 samples\n",
      "Added  544 samples\n",
      "Added  545 samples\n",
      "Added  546 samples\n",
      "Added  547 samples\n",
      "Added  548 samples\n",
      "Added  549 samples\n",
      "Added  550 samples\n",
      "Added  551 samples\n",
      "Added  552 samples\n",
      "Added  553 samples\n",
      "Added  554 samples\n",
      "Added  555 samples\n",
      "Added  556 samples\n",
      "Added  557 samples\n",
      "Added  558 samples\n",
      "Added  559 samples\n",
      "Added  560 samples\n",
      "Added  561 samples\n",
      "Added  562 samples\n",
      "Added  563 samples\n",
      "Added  564 samples\n",
      "Added  565 samples\n",
      "Added  566 samples\n",
      "Added  567 samples\n",
      "Added  568 samples\n",
      "Added  569 samples\n",
      "Added  570 samples\n",
      "Added  571 samples\n",
      "Added  572 samples\n",
      "Added  573 samples\n",
      "Added  574 samples\n",
      "Added  575 samples\n",
      "Added  576 samples\n",
      "Added  577 samples\n",
      "Added  578 samples\n",
      "Added  579 samples\n",
      "Added  580 samples\n",
      "Added  581 samples\n",
      "Added  582 samples\n",
      "Added  583 samples\n",
      "Added  584 samples\n",
      "Added  585 samples\n",
      "Added  586 samples\n",
      "Added  587 samples\n",
      "Added  588 samples\n",
      "Added  589 samples\n",
      "Added  590 samples\n",
      "Added  591 samples\n",
      "Added  592 samples\n",
      "Added  593 samples\n",
      "Added  594 samples\n",
      "Added  595 samples\n",
      "Added  596 samples\n",
      "Added  597 samples\n",
      "Added  598 samples\n",
      "Added  599 samples\n",
      "Added  600 samples\n",
      "Added  601 samples\n",
      "Added  602 samples\n",
      "Added  603 samples\n",
      "Added  604 samples\n",
      "Added  605 samples\n",
      "Added  606 samples\n",
      "Added  607 samples\n",
      "Added  608 samples\n",
      "Added  609 samples\n",
      "Added  610 samples\n",
      "Added  611 samples\n",
      "Added  612 samples\n",
      "Added  613 samples\n",
      "Added  614 samples\n",
      "Added  615 samples\n",
      "Added  616 samples\n",
      "Added  617 samples\n",
      "Added  618 samples\n",
      "Added  619 samples\n",
      "Added  620 samples\n",
      "Added  621 samples\n",
      "Added  622 samples\n",
      "Added  623 samples\n",
      "Added  624 samples\n",
      "Added  625 samples\n",
      "Added  626 samples\n",
      "Added  627 samples\n",
      "Added  628 samples\n",
      "Added  629 samples\n",
      "Added  630 samples\n",
      "Added  631 samples\n",
      "Added  632 samples\n",
      "Added  633 samples\n",
      "Added  634 samples\n",
      "Added  635 samples\n",
      "Added  636 samples\n",
      "Added  637 samples\n",
      "Added  638 samples\n",
      "Added  639 samples\n",
      "Added  640 samples\n",
      "Added  641 samples\n",
      "Added  642 samples\n",
      "Added  643 samples\n",
      "Added  644 samples\n",
      "Added  645 samples\n",
      "Added  646 samples\n",
      "Added  647 samples\n",
      "Added  648 samples\n",
      "Added  649 samples\n",
      "Added  650 samples\n",
      "Added  651 samples\n",
      "Added  652 samples\n",
      "Added  653 samples\n",
      "Added  654 samples\n",
      "Added  655 samples\n",
      "Added  656 samples\n",
      "Added  657 samples\n",
      "Added  658 samples\n",
      "Added  659 samples\n",
      "Added  660 samples\n",
      "Added  661 samples\n",
      "Added  662 samples\n",
      "Added  663 samples\n",
      "Added  664 samples\n",
      "Added  665 samples\n",
      "Added  666 samples\n",
      "Added  667 samples\n",
      "Added  668 samples\n",
      "Added  669 samples\n",
      "Added  670 samples\n",
      "Added  671 samples\n",
      "Added  672 samples\n",
      "Added  673 samples\n",
      "Added  674 samples\n",
      "Added  675 samples\n",
      "Added  676 samples\n",
      "Added  677 samples\n",
      "Added  678 samples\n",
      "Added  679 samples\n",
      "Added  680 samples\n",
      "Added  681 samples\n",
      "Added  682 samples\n",
      "Added  683 samples\n",
      "Added  684 samples\n",
      "Added  685 samples\n",
      "Added  686 samples\n",
      "Added  687 samples\n",
      "Added  688 samples\n",
      "Added  689 samples\n",
      "Added  690 samples\n",
      "Added  691 samples\n",
      "Added  692 samples\n",
      "Added  693 samples\n",
      "Added  694 samples\n",
      "Added  695 samples\n",
      "Added  696 samples\n",
      "Added  697 samples\n",
      "Added  698 samples\n",
      "Added  699 samples\n",
      "Added  700 samples\n",
      "Added  701 samples\n",
      "Added  702 samples\n",
      "Added  703 samples\n",
      "Added  704 samples\n",
      "Added  705 samples\n",
      "Added  706 samples\n",
      "Added  707 samples\n",
      "Added  708 samples\n",
      "Added  709 samples\n",
      "Added  710 samples\n",
      "Added  711 samples\n",
      "Added  712 samples\n",
      "Added  713 samples\n",
      "Added  714 samples\n",
      "Added  715 samples\n",
      "Added  716 samples\n",
      "Added  717 samples\n",
      "Added  718 samples\n",
      "Added  719 samples\n",
      "Added  720 samples\n",
      "Added  721 samples\n",
      "Added  722 samples\n",
      "Added  723 samples\n",
      "Added  724 samples\n",
      "Added  725 samples\n",
      "Added  726 samples\n",
      "Added  727 samples\n",
      "Added  728 samples\n",
      "Added  729 samples\n",
      "Added  730 samples\n",
      "Added  731 samples\n",
      "Added  732 samples\n",
      "Added  733 samples\n",
      "Added  734 samples\n",
      "Added  735 samples\n",
      "Added  736 samples\n",
      "Added  737 samples\n",
      "Added  738 samples\n",
      "Added  739 samples\n",
      "Added  740 samples\n",
      "Added  741 samples\n",
      "Added  742 samples\n",
      "Added  743 samples\n",
      "Added  744 samples\n",
      "Added  745 samples\n",
      "Added  746 samples\n",
      "Added  747 samples\n",
      "Added  748 samples\n",
      "Added  749 samples\n",
      "Added  750 samples\n",
      "Added  751 samples\n",
      "Added  752 samples\n",
      "Added  753 samples\n",
      "Added  754 samples\n",
      "Added  755 samples\n",
      "Added  756 samples\n",
      "Added  757 samples\n",
      "Added  758 samples\n",
      "Added  759 samples\n",
      "Added  760 samples\n",
      "Added  761 samples\n",
      "Added  762 samples\n",
      "Added  763 samples\n",
      "Added  764 samples\n",
      "Added  765 samples\n",
      "Added  766 samples\n",
      "Added  767 samples\n",
      "Added  768 samples\n",
      "Added  769 samples\n",
      "Added  770 samples\n",
      "Added  771 samples\n",
      "Added  772 samples\n",
      "Added  773 samples\n",
      "Added  774 samples\n",
      "Added  775 samples\n",
      "Added  776 samples\n",
      "Added  777 samples\n",
      "Added  778 samples\n",
      "Added  779 samples\n",
      "Added  780 samples\n",
      "Added  781 samples\n",
      "Added  782 samples\n",
      "Added  783 samples\n",
      "Added  784 samples\n",
      "Added  785 samples\n",
      "Added  786 samples\n",
      "Added  787 samples\n",
      "Added  788 samples\n",
      "Added  789 samples\n",
      "Added  790 samples\n",
      "Added  791 samples\n",
      "Added  792 samples\n",
      "Added  793 samples\n",
      "Added  794 samples\n",
      "Added  795 samples\n",
      "Added  796 samples\n",
      "Added  797 samples\n",
      "Added  798 samples\n",
      "Added  799 samples\n",
      "Added  800 samples\n",
      "Added  801 samples\n",
      "Added  802 samples\n",
      "Added  803 samples\n",
      "Added  804 samples\n",
      "Added  805 samples\n",
      "Added  806 samples\n",
      "Added  807 samples\n",
      "Added  808 samples\n",
      "Added  809 samples\n",
      "Added  810 samples\n",
      "Added  811 samples\n",
      "Added  812 samples\n",
      "Added  813 samples\n",
      "Added  814 samples\n",
      "Added  815 samples\n",
      "Added  816 samples\n",
      "Added  817 samples\n",
      "Added  818 samples\n",
      "Added  819 samples\n",
      "Added  820 samples\n",
      "Added  821 samples\n",
      "Added  822 samples\n",
      "Added  823 samples\n",
      "Added  824 samples\n",
      "Added  825 samples\n",
      "Added  826 samples\n",
      "Added  827 samples\n",
      "Added  828 samples\n",
      "Added  829 samples\n",
      "Added  830 samples\n",
      "Added  831 samples\n",
      "Added  832 samples\n",
      "Added  833 samples\n",
      "Added  834 samples\n",
      "Added  835 samples\n",
      "Added  836 samples\n",
      "Added  837 samples\n",
      "Added  838 samples\n",
      "Added  839 samples\n",
      "Added  840 samples\n",
      "Added  841 samples\n",
      "Added  842 samples\n",
      "Added  843 samples\n",
      "Added  844 samples\n",
      "Added  845 samples\n",
      "Added  846 samples\n",
      "Added  847 samples\n",
      "Added  848 samples\n",
      "Added  849 samples\n",
      "Added  850 samples\n",
      "Added  851 samples\n",
      "Added  852 samples\n",
      "Added  853 samples\n",
      "Added  854 samples\n",
      "Added  855 samples\n",
      "Added  856 samples\n",
      "Added  857 samples\n",
      "Added  858 samples\n",
      "Added  859 samples\n",
      "Added  860 samples\n",
      "Added  861 samples\n",
      "Added  862 samples\n",
      "Added  863 samples\n",
      "Added  864 samples\n",
      "Added  865 samples\n",
      "Added  866 samples\n",
      "Added  867 samples\n",
      "Added  868 samples\n",
      "Added  869 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added  870 samples\n",
      "Added  871 samples\n",
      "Added  872 samples\n",
      "Added  873 samples\n",
      "Added  874 samples\n",
      "Added  875 samples\n",
      "Added  876 samples\n",
      "Added  877 samples\n",
      "Added  878 samples\n",
      "Added  879 samples\n",
      "Added  880 samples\n",
      "Added  881 samples\n",
      "Added  882 samples\n",
      "Added  883 samples\n",
      "Added  884 samples\n",
      "Added  885 samples\n",
      "Added  886 samples\n",
      "Added  887 samples\n",
      "Added  888 samples\n",
      "Added  889 samples\n",
      "Added  890 samples\n",
      "Added  891 samples\n",
      "Added  892 samples\n",
      "Added  893 samples\n",
      "Added  894 samples\n",
      "Added  895 samples\n",
      "Added  896 samples\n",
      "Added  897 samples\n",
      "Added  898 samples\n",
      "Added  899 samples\n",
      "Added  900 samples\n",
      "Added  901 samples\n",
      "Added  902 samples\n",
      "Added  903 samples\n",
      "Added  904 samples\n",
      "Added  905 samples\n",
      "Added  906 samples\n",
      "Added  907 samples\n",
      "Added  908 samples\n",
      "Added  909 samples\n",
      "Added  910 samples\n",
      "Added  911 samples\n",
      "Added  912 samples\n",
      "Added  913 samples\n",
      "Added  914 samples\n",
      "Added  915 samples\n",
      "Added  916 samples\n",
      "Added  917 samples\n",
      "Added  918 samples\n",
      "Added  919 samples\n",
      "Added  920 samples\n",
      "Added  921 samples\n",
      "Added  922 samples\n",
      "Added  923 samples\n",
      "Added  924 samples\n",
      "Added  925 samples\n",
      "Added  926 samples\n",
      "Added  927 samples\n",
      "Added  928 samples\n",
      "Added  929 samples\n",
      "Added  930 samples\n",
      "Added  931 samples\n",
      "Added  932 samples\n",
      "Added  933 samples\n",
      "Added  934 samples\n",
      "Added  935 samples\n",
      "Added  936 samples\n",
      "Added  937 samples\n",
      "Added  938 samples\n",
      "Added  939 samples\n",
      "Added  940 samples\n",
      "Added  941 samples\n",
      "Added  942 samples\n",
      "Added  943 samples\n",
      "Added  944 samples\n",
      "Added  945 samples\n",
      "Added  946 samples\n",
      "Added  947 samples\n",
      "Added  948 samples\n",
      "Added  949 samples\n",
      "Added  950 samples\n",
      "Added  951 samples\n",
      "Added  952 samples\n",
      "Added  953 samples\n",
      "Added  954 samples\n",
      "Added  955 samples\n",
      "Added  956 samples\n",
      "Added  957 samples\n",
      "Added  958 samples\n",
      "Added  959 samples\n",
      "Added  960 samples\n",
      "Added  961 samples\n",
      "Added  962 samples\n",
      "Added  963 samples\n",
      "Added  964 samples\n",
      "Added  965 samples\n",
      "Added  966 samples\n",
      "Added  967 samples\n",
      "Added  968 samples\n",
      "Added  969 samples\n",
      "Added  970 samples\n",
      "Added  971 samples\n",
      "Added  972 samples\n",
      "Added  973 samples\n",
      "Added  974 samples\n",
      "Added  975 samples\n",
      "Added  976 samples\n",
      "Added  977 samples\n",
      "Added  978 samples\n",
      "Added  979 samples\n",
      "Added  980 samples\n",
      "Added  981 samples\n",
      "Added  982 samples\n",
      "Added  983 samples\n",
      "Added  984 samples\n",
      "Added  985 samples\n",
      "Added  986 samples\n",
      "Added  987 samples\n",
      "Added  988 samples\n",
      "Added  989 samples\n",
      "Added  990 samples\n",
      "Added  991 samples\n",
      "Added  992 samples\n",
      "Added  993 samples\n",
      "Added  994 samples\n",
      "Added  995 samples\n",
      "Added  996 samples\n",
      "Added  997 samples\n",
      "Added  998 samples\n",
      "Added  999 samples\n",
      "Added  1000 samples\n",
      "Added  1001 samples\n",
      "Added  1002 samples\n",
      "Added  1003 samples\n",
      "Added  1004 samples\n",
      "Added  1005 samples\n",
      "Added  1006 samples\n",
      "Added  1007 samples\n",
      "Added  1008 samples\n",
      "Added  1009 samples\n",
      "Added  1010 samples\n",
      "Added  1011 samples\n",
      "Added  1012 samples\n",
      "Added  1013 samples\n",
      "Added  1014 samples\n",
      "Added  1015 samples\n",
      "Added  1016 samples\n",
      "Added  1017 samples\n",
      "Added  1018 samples\n",
      "Added  1019 samples\n",
      "Added  1020 samples\n",
      "Added  1021 samples\n",
      "Added  1022 samples\n",
      "Added  1023 samples\n",
      "Added  1024 samples\n",
      "Added  1025 samples\n",
      "Added  1026 samples\n",
      "Added  1027 samples\n",
      "Added  1028 samples\n",
      "Added  1029 samples\n",
      "Added  1030 samples\n",
      "Added  1031 samples\n",
      "Added  1032 samples\n",
      "Added  1033 samples\n",
      "Added  1034 samples\n",
      "Added  1035 samples\n",
      "Added  1036 samples\n",
      "Added  1037 samples\n",
      "Added  1038 samples\n",
      "Added  1039 samples\n",
      "Added  1040 samples\n",
      "Added  1041 samples\n",
      "Added  1042 samples\n",
      "Added  1043 samples\n",
      "Added  1044 samples\n",
      "Added  1045 samples\n",
      "Added  1046 samples\n",
      "Added  1047 samples\n",
      "Added  1048 samples\n",
      "Added  1049 samples\n",
      "Added  1050 samples\n",
      "Added  1051 samples\n",
      "Added  1052 samples\n",
      "Added  1053 samples\n",
      "Added  1054 samples\n",
      "Added  1055 samples\n",
      "Added  1056 samples\n",
      "Added  1057 samples\n",
      "Added  1058 samples\n",
      "Added  1059 samples\n",
      "Added  1060 samples\n",
      "Added  1061 samples\n",
      "Added  1062 samples\n",
      "Added  1063 samples\n",
      "Added  1064 samples\n",
      "Added  1065 samples\n",
      "Added  1066 samples\n",
      "Added  1067 samples\n",
      "Added  1068 samples\n",
      "Added  1069 samples\n",
      "Added  1070 samples\n",
      "Added  1071 samples\n",
      "Added  1072 samples\n",
      "Added  1073 samples\n",
      "Added  1074 samples\n",
      "Added  1075 samples\n",
      "Added  1076 samples\n",
      "Added  1077 samples\n",
      "Added  1078 samples\n",
      "Added  1079 samples\n",
      "Added  1080 samples\n",
      "Added  1081 samples\n",
      "Added  1082 samples\n",
      "Added  1083 samples\n",
      "Added  1084 samples\n",
      "Added  1085 samples\n",
      "Added  1086 samples\n",
      "Added  1087 samples\n",
      "Added  1088 samples\n",
      "Added  1089 samples\n",
      "Added  1090 samples\n",
      "Added  1091 samples\n",
      "Added  1092 samples\n",
      "Added  1093 samples\n",
      "Added  1094 samples\n",
      "Added  1095 samples\n",
      "Added  1096 samples\n",
      "Added  1097 samples\n",
      "Added  1098 samples\n",
      "Added  1099 samples\n",
      "Added  1100 samples\n",
      "Added  1101 samples\n",
      "Added  1102 samples\n",
      "Added  1103 samples\n",
      "Added  1104 samples\n",
      "Added  1105 samples\n",
      "Added  1106 samples\n",
      "Added  1107 samples\n",
      "Added  1108 samples\n",
      "Added  1109 samples\n",
      "Added  1110 samples\n",
      "Added  1111 samples\n",
      "Added  1112 samples\n",
      "Added  1113 samples\n",
      "Added  1114 samples\n",
      "Added  1115 samples\n",
      "Added  1116 samples\n",
      "Added  1117 samples\n",
      "Added  1118 samples\n",
      "Added  1119 samples\n",
      "Added  1120 samples\n"
     ]
    }
   ],
   "source": [
    "data_final = defaultdict(list)\n",
    "\n",
    "count=1\n",
    "\n",
    "for flair_key in flair_values:\n",
    "    search_flair = subreddit.search(flair_values[flair_key], limit=100)\n",
    "    for post in search_flair:\n",
    "        data_final['flair'].append(flair_key)\n",
    "        data_final['title'].append(post.title)\n",
    "        data_final['url'].append(post.url)\n",
    "        data_final['text'].append(post.selftext)\n",
    "        data_final['score'].append(post.score)\n",
    "        data_final['num_comments'].append(post.num_comments)\n",
    "        data_final['original'].append(post.is_original_content)\n",
    "        \n",
    "        post.comments.replace_more(limit=None)\n",
    "        all_comments = ''\n",
    "        for comment in post.comments:\n",
    "            all_comments = all_comments + ' ' + comment.body\n",
    "\n",
    "        data_final['comments'].append(all_comments)\n",
    "        \n",
    "        print('Added ', count, 'samples')\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>original</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ethereum Fees Now at July 2021 Lows of $2.54 -...</td>\n",
       "      <td>https://cryptonewsland.com/ethereum-fees-now-a...</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>Ethereum [pros](/r/CryptoMarkets/comments/ux7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Coinbase CEO’s Response to Employee Rant Backf...</td>\n",
       "      <td>https://cryptonewsland.com/coinbase-ceos-respo...</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>USDC [pros](/r/CryptoMarkets/comments/v9izys/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flair                                              title  \\\n",
       "0      0  Ethereum Fees Now at July 2021 Lows of $2.54 -...   \n",
       "1      0  Coinbase CEO’s Response to Employee Rant Backf...   \n",
       "\n",
       "                                                 url text  score  \\\n",
       "0  https://cryptonewsland.com/ethereum-fees-now-a...          21   \n",
       "1  https://cryptonewsland.com/coinbase-ceos-respo...           4   \n",
       "\n",
       "   num_comments  original                                           comments  \n",
       "0            14     False   Ethereum [pros](/r/CryptoMarkets/comments/ux7...  \n",
       "1             7     False   USDC [pros](/r/CryptoMarkets/comments/v9izys/...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SIA()\n",
    "results = []\n",
    "\n",
    "for line in df_final['comments']:\n",
    "    pol_score = sia.polarity_scores(line)\n",
    "    pol_score['comments'] = line\n",
    "    results.append(pol_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>Ethereum [pros](/r/CryptoMarkets/comments/ux7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.168</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.4404</td>\n",
       "      <td>USDC [pros](/r/CryptoMarkets/comments/v9izys/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.2732</td>\n",
       "      <td>Bitcoin [pros](/r/CryptoMarkets/comments/v9vc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>Do Kwan screwed us. They were just looking fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.8979</td>\n",
       "      <td>Inflation [pros](/r/CryptoMarkets/comments/vd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound  \\\n",
       "0  0.038  0.785  0.177    0.9497   \n",
       "1  0.168  0.759  0.073   -0.4404   \n",
       "2  0.095  0.905  0.000   -0.2732   \n",
       "3  0.031  0.877  0.092    0.6705   \n",
       "4  0.058  0.759  0.183    0.8979   \n",
       "\n",
       "                                            comments  \n",
       "0   Ethereum [pros](/r/CryptoMarkets/comments/ux7...  \n",
       "1   USDC [pros](/r/CryptoMarkets/comments/v9izys/...  \n",
       "2   Bitcoin [pros](/r/CryptoMarkets/comments/v9vc...  \n",
       "3   Do Kwan screwed us. They were just looking fo...  \n",
       "4   Inflation [pros](/r/CryptoMarkets/comments/vd...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.join(df2['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.rename(columns = {'compound':'polarity'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>original</th>\n",
       "      <th>comments</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Ethereum Fees Now at July 2021 Lows of $2.54 -...</td>\n",
       "      <td>https://cryptonewsland.com/ethereum-fees-now-a...</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>Ethereum [pros](/r/CryptoMarkets/comments/ux7...</td>\n",
       "      <td>0.9497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Coinbase CEO’s Response to Employee Rant Backf...</td>\n",
       "      <td>https://cryptonewsland.com/coinbase-ceos-respo...</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>USDC [pros](/r/CryptoMarkets/comments/v9izys/...</td>\n",
       "      <td>-0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Mars4 Metaverse is Selling Fast in Japan – Spo...</td>\n",
       "      <td>https://news.bitcoin.com/mars4-metaverse-is-se...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>Bitcoin [pros](/r/CryptoMarkets/comments/v9vc...</td>\n",
       "      <td>-0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Terra Crisis: G-7 Nations Call for Urgent Cryp...</td>\n",
       "      <td>https://crypto.news/terra-g-7-nations-urgent-c...</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>Do Kwan screwed us. They were just looking fo...</td>\n",
       "      <td>0.6705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>White House says the Administration is Closely...</td>\n",
       "      <td>https://www.youtube.com/watch?v=4oSL1gCHFdQ</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>Inflation [pros](/r/CryptoMarkets/comments/vd...</td>\n",
       "      <td>0.8979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flair                                              title  \\\n",
       "0      0  Ethereum Fees Now at July 2021 Lows of $2.54 -...   \n",
       "1      0  Coinbase CEO’s Response to Employee Rant Backf...   \n",
       "2      0  Mars4 Metaverse is Selling Fast in Japan – Spo...   \n",
       "3      0  Terra Crisis: G-7 Nations Call for Urgent Cryp...   \n",
       "4      0  White House says the Administration is Closely...   \n",
       "\n",
       "                                                 url text  score  \\\n",
       "0  https://cryptonewsland.com/ethereum-fees-now-a...          21   \n",
       "1  https://cryptonewsland.com/coinbase-ceos-respo...           4   \n",
       "2  https://news.bitcoin.com/mars4-metaverse-is-se...           2   \n",
       "3  https://crypto.news/terra-g-7-nations-urgent-c...          14   \n",
       "4        https://www.youtube.com/watch?v=4oSL1gCHFdQ           0   \n",
       "\n",
       "   num_comments  original                                           comments  \\\n",
       "0            14     False   Ethereum [pros](/r/CryptoMarkets/comments/ux7...   \n",
       "1             7     False   USDC [pros](/r/CryptoMarkets/comments/v9izys/...   \n",
       "2             9     False   Bitcoin [pros](/r/CryptoMarkets/comments/v9vc...   \n",
       "3            13     False   Do Kwan screwed us. They were just looking fo...   \n",
       "4             4     False   Inflation [pros](/r/CryptoMarkets/comments/vd...   \n",
       "\n",
       "   polarity  \n",
       "0    0.9497  \n",
       "1   -0.4404  \n",
       "2   -0.2732  \n",
       "3    0.6705  \n",
       "4    0.8979  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('final_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
